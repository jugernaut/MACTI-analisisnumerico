{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Envoltorios.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/MACTI-analisisnumerico/blob/main/09_ProgramacionParalelo/CPU/03_Envoltorios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCy4sQbk6TDS"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Envoltorios (Wrappers)</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M. en C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jonathan Ramírez Montes</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Manejo de Datos</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaIeAtF6T1G"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Una vez que ya se conocen las principales *API's* para programar en paralelo como *OpenMP*, *MPI* o *CUDA*, así como sus ventajas y desventajas podemos comenzar a utilizar alternativas como lo son los ***wrappers*** (envoltorios).\n",
        "\n",
        "Un *wrapper* es un conjunto de librerías y herramientas (en otro lenguaje diferente a *C/C++*) que actúa como puente y oculta muchos de los detalles de este tipo de *API's*.\n",
        "\n",
        "Existe una infinidad de lenguajes de alto nivel que permiten hacer uso de estos *wrappers*, como lo son *JAVA*, *Python*, *R*, etc.\n",
        "\n",
        "Para esta presentación nos enfocaremos en el lenguaje *Python* y algunos de los *wrappers* que existen en este lenguaje ya que las ventajas que ofrece este lenguaje lo hacen ideal para su uso en este curso.\n",
        "\n",
        "Dos de los envoltorios más populares para *Python* son *Numba* y *TensorFlow*.\n",
        "\n",
        "A pesar de la gran cantidad de *wrappers* que existen actualmente, debido a los alcances del curso, solo podremos revisar *Numba* y *TensorFlow*.\n",
        "\n",
        "Aquí podemos ver las diferentes capas que se construyen con *Python*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/wrapper.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Los 2 diferentes enfoques que se les puede dar a los envoltorios.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/arribabajo.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Flujo de *Numba*, ¿cómo es que se optimiza el código?.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba-arch.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Relación entre *TensorFlow* y *Nvidia*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor2.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor1.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Capas en el desarrollo de *software*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensordevice.jpg?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxaDk7Y_8D-"
      },
      "source": [
        "## *CPU* v.s. *GPU*\n",
        "\n",
        "Actualmente los dispositivos de cómputo contienen al menos un *CPU* y dentro de este *CPU* pueden estar contenidos varios núcleos, lo que permite el desarrollo de algoritmos en paralelo.\n",
        "\n",
        "De igual manera, la mayoría de los dispositivos de cómputo contienen al menos un *GPU* y dentro de este *GPU* pueden existir varios núcleos, la principal diferencia entre ambos (*CPU* y *GPU*) es el propósito para el cuál fueron diseñados\n",
        "\n",
        "Para fines prácticos (y del curso) podemos pensar que la diferencia principal entre una *CPU* (unidad de procesamiento central) y una *GPU* (**unidad de procesamiento gráfico**) radica en que un *CPU* es un dispositivo de cómputo de propósito general, puede realizar cualquier tipo de cómputo que se le asigne.\n",
        "\n",
        "Por otro lado un *GPU* esta diseñado para el procesamiento gráfico, lo que significa que la forma en la que procesa información **esta optimizada para trabajar con matrices y vectores**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/MonteCarlo/cpu-vs-gpu.jpg?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/MonteCarlo/mejor.png?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PPn3oSp8WaM"
      },
      "source": [
        "# ¿Qué es *Numba*?\n",
        "\n",
        "Según la documentación de *Numba*, \"*Numba* es un compilador justo a tiempo para *Python* que funciona mejor en código que usa matrices, funciones de *Numpy* y ciclos.\n",
        "\n",
        "En otras palabras, *Numba* es una paquetería (igual que *Numpy* o *Matplotlib*) que nos ayuda a que nuestros algoritmos se ejecuten de forma optimizada y en particular nos permite tener acceso a los *GPU's* disponibles en el hardware que estemos ejecutando nuestros algoritmos.\n",
        "\n",
        "La forma más común de usar *Numba* es a través de su colección de *decoradores* que se pueden aplicar a sus funciones para indicarle a *Numba* que las compile. Cuando se realiza una llamada a una función decorada de *Numba*, **se compila en código máquina** \"justo a tiempo\" para su ejecución y todo o parte de su código puede ejecutarse posteriormente a la ¡velocidad del código máquina nativo!.\n",
        "\n",
        "Para nuestros propósitos de hoy, *Numba* es un paquete de *Python* que le permite escribir código *Python* para *GPU*, y tambien puede usarse para programación en paralelo mediante *CPU's*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQuAQn-eZmc_"
      },
      "source": [
        "## Dependencia Funcional\n",
        "\n",
        "Existen muchas formas para poder identificar si existe la posibilidad de emplear alguna técnica de paralelización algun algoritmo.\n",
        "\n",
        "Sin embargo la forma más sencilla de saber cuando NO ES POSIBLE programar algún algoritmo en su equivalente en paralelo, es mediante las dependencias funcionales.\n",
        "\n",
        "La idea de **dependencia funcional** es la generalización del concepto de **dependencia lineal**. Se dice que un conjunto de funciones es **funcionalmente dependiente** si existe una relación funcional entre ellas, en otras palabras cuando alguna de las funciones del conjunto es expresable como función de las otras funciones definidas previamente dentro del conjunto.\n",
        "\n",
        "Esto significa que cuando existe una dependencia funcional, para poder obtener el resultado del algoritmo $f_n(f_{n-1}(x))$ es necesario esperar el resultado de $f_{n-1}(x)$, lo que implica que este tipo de algoritmos no es candidato a ser programado en paralelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOiTvC5u8cQP"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "*Numba* tiene multiples formás de optimizar codigo y lograr que este muestre un mejor desempeño, esto lo realiza mediante alguna de las siguientes variantes:\n",
        "\n",
        "• Convierte código *Python* en código de máquina: al compilar código empleando *Numba*, este convierte el código en código de máquina y la segunda vez que sea ejecutado este se ejecuta en lenguaje de muy bajo nivel que se traduce en una ejecución más rápida.\n",
        "\n",
        "• Es posible utilizar una capa (*layer*) para acceder a características de *OpenMP*.\n",
        "\n",
        "• Es posible paralelizar código empleando utilidades de *MPI*.\n",
        "\n",
        "• Tiene soporte para el uso de *GPU's* utilizando *CUDA* como *background*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehjbuHF8sKR"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "*Numba* posee múltiples ventajas, aunque una de las más importantes es poder decidir como optimizar el código escrito en *Python*.\n",
        "\n",
        "Es muy sencillo de instalar mediante *pip* e igual de fácil de usar que *Python*.\n",
        "\n",
        "Se tiene una gran capacidad de acoplamiento con *Numpy* (biblioteca para cómputo científico).\n",
        "\n",
        "Además de ser posible optar por un mecanismo para optimizar el código, *Numba* permite escribir código híbrido que combine lo mejor de las diferentes formás de optimizar el desempeño.\n",
        "\n",
        "Emplear *Numba* es tan sencillo como importar la biblioteca y hacer uso de sus **decoradores** para optimizar el código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhW1gUa9DdF"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "*Numba* tiene en realidad muy pocas desventajas.\n",
        "\n",
        "La más evidente de estas es que **encapsula mucho de su funcionamiento**, es decir que en realidad funciona como caja negra.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg_-5MXa94pG"
      },
      "source": [
        "### Decoradores\n",
        "\n",
        "Un decorador en *Numba* es una forma de modificar funciones de manera tal que pueda ser optimizada empleando alguna de las técnicas previamente mencionadas.\n",
        "\n",
        "Se puede pensar en un decorador en una función que recibe una función como parámetro y devuelve otra función optimizada como salida.\n",
        "\n",
        "Una función de *Python* es envuelta por uno o más decoradores, una vez que se define esta función el decorador es evaluado y *Numba* devuelve una función optimizada que puede ser invocada desde *Python*.\n",
        "\n",
        "El alcance del ó de los decoradores se limita al alcance de la función definida a la cual se le aplique dichos decoradores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7J7oOHfwBeM"
      },
      "source": [
        "# Sintaxis *Numba*\n",
        "\n",
        "Al hacer uso de *Numba* (*CUDA*), debemos tomar en cuenta que la finalidad de *Numba* es acelerar el desempeño de nuestros algoritmos.\n",
        "\n",
        "Para llevar a cabo tal propósito, *Numba* provee de sentencias que se agregan al código de *Python* conocidos como *decoradores*. Estos decoradores son de diferentes tipos, ya que *Numba* permite acelerar el desempeño de diferentes formas.\n",
        "\n",
        "Mediante *Numba* los cálculos de nuestros algoritmos se pueden distribuir entre el *CPU* y el *GPU*, es por eso que *Numba* nos ofrece una forma para distinguir cuáles de nuestras funciones se ejecutan en uno u otro dispositivo de cómputo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlujxYcPPsor"
      },
      "source": [
        "### `nopython=True`\n",
        "\n",
        "El decorador más sencillo de usar en *Numba* es el decorador `@jit` (*Just in time*) y permite recibir multiples parametros que indican la forma en la que se quiere optimizar el código (ver referencias). Si se usa este decorador sin parámetros *Numba* decide cuál es la mejor forma para optimizar el desempeño del algoritmo.\n",
        "\n",
        "Uno de sus parámetros más elementales pero al mismo tiempo uno de los más usados, es el parametro `nopython=True`.\n",
        "\n",
        "Al hacer uso de este decorador se le indica a *Numba* que el código que pertenece a esta definición debe ser optimizado y esto se lleva a cabo, convirtiendo el código de python en **código de máquina**, esto toma un poco más de tiempo la primera vez que se ejecuta, pero las siguientes veces tomará mucho menos tiempo que una función en *Python* nativo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8tNuU3pjVAK"
      },
      "source": [
        "import random\n",
        "# biblioteca de Numba\n",
        "from numba import jit\n",
        "\n",
        "#¡¡¡descomentar para optimizar, te vas a sorprender!!!\n",
        "@jit(nopython=True)\n",
        "def mc_pi_aprox(n=100000000):\n",
        "    dentro_circulo = 0 \n",
        "    for i in range(n):\n",
        "      x = random.random()\n",
        "      y = random.random()\n",
        "      # valores dentro de la circunferencia\n",
        "      if (x**2+y**2 < 1):\n",
        "          dentro_circulo += 1\n",
        "    return 4*dentro_circulo / n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLldJZM95rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c392dfd6-c9b9-464e-ee62-e1d524b1a14e"
      },
      "source": [
        "%%time\n",
        "mc_pi_aprox()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.09 s, sys: 1.05 ms, total: 1.09 s\n",
            "Wall time: 1.09 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.14137392"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8LzorbuqUr2"
      },
      "source": [
        "En el ejemplo de la celda superior en la cuál se aproxima el valor de $\\pi$ mediante Monte Carlo, al ejecutarlo sin el uso de *Numba* (solo *Python* nativo), podemos notar que el tiempo de ejecución se incrementa de manera proporcional a la precisión que usemos. \n",
        "\n",
        "Por otro lado, si se usa el decorador *@jit(nopython=True)*, qué es equivalente a *@njit*, de manera explicita se le pide a *Numba*, que convierta el **código a código de máquina**. Y el tiempo de ejecución disminuye de manera notable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiMtF_AR1Vg2"
      },
      "source": [
        "### `parallel` (*OpenMP*)\n",
        "\n",
        "Otro parámetro muy útil pero a la vez 'obscuro' es, `@njit(parrallel=True)`.\n",
        "\n",
        "Este decorador va de la mano de la palabra reservada `prange` y en conjunto permiten ejecutar en paralelo ciclos dentro de la función definida.\n",
        "\n",
        "Este decorador oculta mucho del proceso que se realiza al ejecutar un algoritmo en paralelo. Sin embargo ya que a esta altura del curso se conoce cual es el transfondo (*OpenMP, MPI, CUDA*), podemos obviar el mismo.\n",
        "\n",
        "La parlabra reservada `prange` se emplea para especificar el ciclo que se quiere realizar en paralelo y no solo eso, también realiza la operación conocida como *reduction* de alguna variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os9I6k8F-qLI"
      },
      "source": [
        "# import de biblioteca calculos numericos\n",
        "import math\n",
        "\n",
        "# aproximacion de Pi mediante sumas de Reimann (version secuencial)\n",
        "def aproxPIsec(n=10000000):\n",
        "    suma=0\n",
        "    # funcion a integrar\n",
        "    f = lambda x: math.sqrt(1-x**2)\n",
        "    delta = 1/n\n",
        "    for i in range(1,n+1):\n",
        "        x = delta*(i)\n",
        "        suma += f(x)*delta\n",
        "    aproximacion = suma*4\n",
        "    return aproximacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6II7oIQY-07G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03069ea6-89b1-4824-f8f6-28e93bf7957a"
      },
      "source": [
        "%%time\n",
        "aproxPIsec(10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.14 ms, sys: 18 µs, total: 5.15 ms\n",
            "Wall time: 15.8 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1413914776113168"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxroKejyBgsj"
      },
      "source": [
        "Usamos la 'función mágica' `%%time` para mostrar el tiempo y el resultado de la ejecución de la celda en la que se hace uso de la versión secuencial de este algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_bAbmcK_HZQ"
      },
      "source": [
        "# import de biblioteca calculos numericos y paralelizacion\n",
        "from numba import njit, prange, set_num_threads\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def aproxPIpar(n=10000000):\n",
        "    suma=0\n",
        "    # funcion a integrar\n",
        "    f = lambda x: math.sqrt(1-x**2)\n",
        "    delta = 1/n\n",
        "    # prange indica que este for debe ser ejecutado en paralelo\n",
        "    for i in range(1,n+1):\n",
        "        x = delta*(i)\n",
        "        suma += f(x)*delta\n",
        "    aproximacion = suma*4\n",
        "    return aproximacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YozECThR_mXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177c89f8-60f4-4958-b02c-cd6bf46b7260"
      },
      "source": [
        "%%time\n",
        "aproxPIpar(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 480 µs, sys: 0 ns, total: 480 µs\n",
            "Wall time: 485 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1395554669110277"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Volumen de una esfera centrada en el origen\n",
        "\n",
        "Aproximar el volúmen de una esfera de radio 1, centrada en el origen, el valor exacto es \n",
        "\n",
        "$$4pi/3*r^3$$\n",
        "\n",
        "Sigue la sugerencia de la [referencia](https://medium.com/@andrew.chamberlain/an-easy-derivation-of-the-volume-of-spheres-formula-45434f2231e9) y aproxime el volumen de la esfera unitaria centrada en el origen."
      ],
      "metadata": {
        "id": "ay3cQzUiDR-_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zTIkMXxENfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaUoB57BB0op"
      },
      "source": [
        "#### Versión en paralelo\n",
        "\n",
        "Hay varios puntos importantes a notar en la versión paralelo del algoritmo de la aproximación del valor de $\\pi$ mediante sumas de Raimann:\n",
        "\n",
        "1.   En la sección de imports, `njit`, `prange`, `set_num_threads`, estos son los elementos de *Numba* que nos permiten la ejecución de este algoritmo en paralelo.\n",
        "2.   El decorador `@njit(parallel=True)` en conjunto con `prange`, nos indican que secciones del código se ejecutará en paralelo.\n",
        "3.   *Numba* oculta la operación de *redcution sum*, de la variable `suma`.\n",
        "4.   El tiempo de ejecución disminuye bastante en la versión en paralelo con respecto a la versión secuencial y no así la precisión, de hecho podemos descomentar la linea inmediata debajo del ciclo `for` para mejorar la precisión del algoritmo al usar el punto medio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_aD2oaChF-"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "* https://numba.pydata.org/numba-doc/latest/user/5minguide.html\n",
        "* https://nyu-cds.github.io/python-numba/01-jit/\n",
        "* https://christophdeil.com/download/2019-07-11_Christoph_Deil_Numba.pdf\n",
        "* https://numba.pydata.org/numba-doc/dev/cuda/kernels.html\n",
        "* Tolga Soyata: GPU Parallel Program Development Using CUDA."
      ]
    }
  ]
}